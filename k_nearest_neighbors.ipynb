{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors Classification from Scratch with NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-nearest neighhbors (KNN) is a machine learning algorithm that can be used both for classification and regression purposes. It falls under the category of supervised learning algorithms that predicts target values for unseen observations. In other words, it operates on labelled datasets and predicts either a class (classification) or a numeric value (regression) for the test data.\n",
    "\n",
    "In this Jupyter notebook, I will be implementing KNN classification algorithm from scratch using NumPy library only. As I have mentioned in my other notebooks, I won't be using an already implemented version of knn algorithm, I will be implementing it with only NumPy and will be using other libraries for creating and using datasets and data visualization purposes.\n",
    "\n",
    "Considering there are many useful resources out there that explain the fundamentals of knn, I want to get into coding part immediately. Let's do some classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the libraries and functions that will be used throughout the implementation:\n",
    "* `numpy`: Obiviously, it will be used for numerical computation of multidimensional arrays as we are heavily dealing with vectors of high dimensions.\n",
    "* `make_classification`: We will use this to create our very own classification dataset. We can decide how many classes and features we want, whether we want samples to be clustered and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(a, b):\n",
    "    return np.sqrt(np.sum((a-b)**2, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In knn algorithm, we will need a function to calculate the distances between training data points and the data that we would like to classify. Here, I've chosen the euclidian distance as it is the widely used type in machine learning applications. One can try classifying with other distance metrics such as Manhattan distance, Chebychev  distance and etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kneighbors(X_test, return_distance=False):\n",
    "       \n",
    "        n_neighbors = 5\n",
    "        dist = []\n",
    "        neigh_ind = []\n",
    "        \n",
    "        point_dist = [euclidian_distance(x_test, X_train) for x_test in X_test]\n",
    "\n",
    "        for row in point_dist:\n",
    "            enum_neigh = enumerate(row)\n",
    "            sorted_neigh = sorted(enum_neigh, key=lambda x: x[1])[:n_neighbors]\n",
    "    \n",
    "            ind_list = [tup[0] for tup in sorted_neigh]\n",
    "            dist_list = [tup[1] for tup in sorted_neigh]\n",
    "    \n",
    "            dist.append(dist_list)\n",
    "            neigh_ind.append(ind_list)\n",
    "        \n",
    "        if return_distance:\n",
    "            return np.array(dist), np.array(neigh_ind)\n",
    "        \n",
    "        return np.array(neigh_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we consider the practical aspect of knn, we try to find the neighbors, the closest data points to the data that we want to classify. Whether a data point is close or not is determined by our euclidian distance function implemented above. Here, the actual value of the distance between data points does not matter, rather, we are interested in the order of those distances. We pick a number as our hyperparameter (k) before the training process and pick k-neareset neighbors to our data points we want to classify during training. Hence, for instance when we say 5-nearest neighbors, we mean the first 5 closest data points. \n",
    "\n",
    "In the `kneighbors` function above, we find the distances between each point in test dataset (the data points we want to classify) and the rest of the dataset, that is the training data. We store those distances in `point_dist` in which each row corresponds to a list of distances between one test data point and all of the training data. Hence, we go over each row, enumerate it and then sort it according to the distances. The reason we enumerate each row is because we don't want to lose the indices of training data points that we calculated the distances with, since we are going to refer them later. \n",
    "\n",
    "Consequently, `sorted_neigh` holds the first k-nearest neighbors of our test data points and they are sorted according to their euclidian distances. We, then, extract indices and distance values from `sorted_neigh` and return them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, weights='uniform'):\n",
    "        \n",
    "        class_num = 3\n",
    "        \n",
    "        if weights=='uniform':\n",
    "            neighbors = kneighbors(X_test)\n",
    "            y_pred = np.array([np.argmax(np.bincount(y_train[neighbor])) for neighbor in neighbors])\n",
    "        \n",
    "            return y_pred \n",
    "    \n",
    "        if weights=='distance':\n",
    "        \n",
    "            dist, neigh_ind = kneighbors(X_test, return_distance=True)\n",
    "        \n",
    "            inv_dist = 1/dist\n",
    "            \n",
    "            mean_inv_dist = inv_dist / np.sum(inv_dist, axis=1)[:, np.newaxis]\n",
    "            \n",
    "            proba = []\n",
    "            \n",
    "            for i, row in enumerate(mean_inv_dist):\n",
    "                \n",
    "                row_pred = self.y_train[neigh_ind[i]]\n",
    "                \n",
    "                for k in range(class_num):\n",
    "                    indices = np.where(row_pred==k)\n",
    "                    prob_ind = np.sum(row[indices])\n",
    "                    proba.append(np.array(prob_ind))\n",
    "        \n",
    "            predict_proba = np.array(proba).reshape(X_test.shape[0], class_num)\n",
    "            \n",
    "            y_pred = np.array([np.argmax(item) for item in predict_proba])\n",
    "            \n",
    "            return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding the k-nearest neighbors, we try to predict the classes that our test data points belong to. Here, we have k neighbors and each neighbor has a vote in deciding the class label. However, voting mechanism may vary according to the chosen criterion. Here, in the `predict` function above, if `weights` are chosen as `uniform` it means that each neighbor has equal vote (weight) in deciding the class label, irrespective of their distances. \n",
    "\n",
    "Let's say we have 5-nearest neighbors of our test data point, 3 of them belonging to class A and 2 of them belonging to class B. We disregard the distances of neighbors and conclude that the test data point belongs to the class A since the majority of neighbors are part of class A. However, if `weights` are chosen as `distance`, then this means the distances of neighbors do matter, indeed. Hence, whichever neighbor that is closest to the test data point has the most weight (vote) proportional to the inverse of their distances. Thereby, regarding the aforementioned example, if those 2 points belonging the class A are a lot closer to the test data point than the other 3 points, then, this fact alone may play a big role in deciding the class label for data point.\n",
    "\n",
    "In `predict` function, it is quite easy to predict the label for data points if `weights` are `uniform`. First, we get the indices of neighbors and then use those indices to get their corresponding class labels from training dataset. Each row in `neighbors` corresponds to the set of neighbors that each test data point has. We then find the occurences of class labels using `numpy`'s `bincount` function and get the index of the maximum occurence which corresponds to the predicted class label. \n",
    "\n",
    "Things get a little messier if we have `weights` chosen as `distance`. In this case, we find the mean inverse of neighbor distances and calculate class probabilities for each test data point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(X_test, y_test):\n",
    "    y_pred = predict(X_test)\n",
    "        \n",
    "    return float(sum(y_pred == y_test))/ float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have implemented the `score` function as a very simple accuracy metric used in classification problems widely. We simply return the percentage of correctly classified labels. It is pretty straightforward! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples = 1000, n_features=2, n_redundant=0, n_informative=2,\n",
    "                             n_clusters_per_class=1, n_classes=3, random_state=21)\n",
    "\n",
    "mu = np.mean(X, 0)\n",
    "sigma = np.std(X, 0)\n",
    "\n",
    "X = (X - mu ) / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to create the dataset that we will be testing our knn algorithm upon. We make use of `sklearn.dataset`'s `make_classification` function to populate the dataset. Afterwards, we normalize each data point by subtracting the mean and then dividing by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.hstack((X, y[:, np.newaxis]))\n",
    "        \n",
    "np.random.shuffle(data)\n",
    "\n",
    "split_rate = 0.7\n",
    "\n",
    "train, test = np.split(data, [int(split_rate*(data.shape[0]))])\n",
    "\n",
    "X_train = train[:,:-1]\n",
    "y_train = train[:, -1]\n",
    "\n",
    "X_test = test[:,:-1]\n",
    "y_test = test[:, -1]\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the dataset, we implement a random split for our training phase, hence obtain our training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[665, 315, 365, 505, 331],\n",
       "       [607, 656, 467, 674, 178],\n",
       "       [211, 407, 642, 565, 266],\n",
       "       ...,\n",
       "       [143, 198, 223, 344,  40],\n",
       "       [609, 250, 389, 314,  99],\n",
       "       [612, 431,  85, 657, 176]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kneighbors(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to test our code of knn implementation. We get the k-nearest neighbors of our test dataset. Do notice that, each row is related to each data point in our test set and elements in each row correspond to the indices of neighbors of the test data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 0, 0, 2, 1, 1, 1, 1, 2, 0, 0, 2, 2,\n",
       "       0, 2, 0, 1, 1, 1, 2, 2, 0, 1, 2, 2, 0, 2, 1, 1, 0, 1, 0, 0, 1, 2,\n",
       "       1, 0, 0, 1, 2, 2, 0, 0, 0, 1, 2, 2, 2, 0, 0, 2, 1, 1, 1, 2, 0, 2,\n",
       "       1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 0, 2, 2, 1, 0, 2, 0, 1, 1, 0,\n",
       "       2, 2, 0, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 1, 2, 2, 0,\n",
       "       1, 1, 2, 2, 2, 2, 2, 2, 2, 0, 1, 0, 1, 1, 1, 1, 0, 0, 2, 0, 2, 0,\n",
       "       0, 2, 2, 2, 1, 0, 2, 2, 1, 0, 2, 0, 2, 0, 1, 0, 2, 0, 1, 2, 2, 1,\n",
       "       1, 1, 0, 0, 2, 2, 1, 2, 2, 1, 1, 2, 0, 2, 2, 1, 1, 1, 0, 0, 2, 2,\n",
       "       0, 0, 1, 2, 1, 2, 0, 2, 1, 1, 0, 0, 1, 1, 0, 2, 0, 2, 0, 1, 0, 2,\n",
       "       1, 2, 1, 1, 1, 1, 0, 1, 2, 0, 2, 1, 2, 2, 0, 0, 1, 0, 0, 2, 1, 2,\n",
       "       2, 0, 1, 2, 1, 0, 2, 2, 0, 2, 2, 1, 2, 1, 1, 2, 1, 0, 0, 0, 2, 0,\n",
       "       2, 1, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 2, 0, 2, 0, 1, 0, 1, 1, 0,\n",
       "       2, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 1, 1, 0, 1, 0, 0, 1, 2, 1, 1, 1,\n",
       "       2, 1, 1, 1, 0, 1, 2, 0, 2, 2, 2, 1, 1, 2])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, `predict` function outputs the predicted class labels of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9766666666666667"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like our implementation did a really good job given the high accuracy score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Implementation of K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class KNearestNeighbors():\n",
    "    def __init__(self, X_train, y_train, n_neighbors=5, weights='uniform'):\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.weights = weights\n",
    "\n",
    "        self.n_classes = 3\n",
    "\n",
    "    def euclidian_distance(self, a, b):\n",
    "        return np.sqrt(np.sum((a - b)**2, axis=1))\n",
    "\n",
    "    def kneighbors(self, X_test, return_distance=False):\n",
    "\n",
    "        dist = []\n",
    "        neigh_ind = []\n",
    "\n",
    "        point_dist = [self.euclidian_distance(x_test, self.X_train) for x_test in X_test]\n",
    "\n",
    "        for row in point_dist:\n",
    "            enum_neigh = enumerate(row)\n",
    "            sorted_neigh = sorted(enum_neigh,\n",
    "                                  key=lambda x: x[1])[:self.n_neighbors]\n",
    "\n",
    "            ind_list = [tup[0] for tup in sorted_neigh]\n",
    "            dist_list = [tup[1] for tup in sorted_neigh]\n",
    "\n",
    "            dist.append(dist_list)\n",
    "            neigh_ind.append(ind_list)\n",
    "\n",
    "        if return_distance:\n",
    "            return np.array(dist), np.array(neigh_ind)\n",
    "\n",
    "        return np.array(neigh_ind)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "\n",
    "        if self.weights == 'uniform':\n",
    "            neighbors = self.kneighbors(X_test)\n",
    "            y_pred = np.array([\n",
    "                np.argmax(np.bincount(self.y_train[neighbor]))\n",
    "                for neighbor in neighbors\n",
    "            ])\n",
    "\n",
    "            return y_pred\n",
    "\n",
    "        if self.weights == 'distance':\n",
    "\n",
    "            dist, neigh_ind = self.kneighbors(X_test, return_distance=True)\n",
    "\n",
    "            inv_dist = 1 / dist\n",
    "\n",
    "            mean_inv_dist = inv_dist / np.sum(inv_dist, axis=1)[:, np.newaxis]\n",
    "\n",
    "            proba = []\n",
    "\n",
    "            for i, row in enumerate(mean_inv_dist):\n",
    "\n",
    "                row_pred = self.y_train[neigh_ind[i]]\n",
    "\n",
    "                for k in range(self.n_classes):\n",
    "                    indices = np.where(row_pred == k)\n",
    "                    prob_ind = np.sum(row[indices])\n",
    "                    proba.append(np.array(prob_ind))\n",
    "\n",
    "            predict_proba = np.array(proba).reshape(X_test.shape[0],\n",
    "                                                    self.n_classes)\n",
    "\n",
    "            y_pred = np.array([np.argmax(item) for item in predict_proba])\n",
    "\n",
    "            return y_pred\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "\n",
    "        return float(sum(y_pred == y_test)) / float(len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing all necessary functions, it is pretty easy to create the class implementation of knn. Here only newly added function is the `__init__` function for our class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Our Implementation with Sklearnâ€™s KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Our Implementation</th>\n",
       "      <th>Sklearn's Implementation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.955556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Our Implementation  Sklearn's Implementation\n",
       "Accuracy            0.955556                  0.955556"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from KNearestNeighbors import KNearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset = load_iris()\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "mu = np.mean(X, 0)\n",
    "sigma = np.std(X, 0)\n",
    "X = (X - mu ) / sigma\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "                X, y, test_size=0.3, random_state=45)\n",
    "\n",
    "our_classifier = KNearestNeighbors(X_train, y_train, n_neighbors=3)\n",
    "sklearn_classifier = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)\n",
    "\n",
    "our_accuracy = our_classifier.score(X_test, y_test)\n",
    "sklearn_accuracy = sklearn_classifier.score(X_test, y_test)\n",
    "\n",
    "pd.DataFrame([[our_accuracy, sklearn_accuracy]],\n",
    "             ['Accuracy'],    \n",
    "             ['Our Implementation', 'Sklearn\\'s Implementation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the accuracy of our own implementation and of `sklearn`'s implementation are the same. That's good news right? We have done a pretty good job with this implementation.\n",
    "\n",
    "Feel free to check out my repository to view my other implementations. \n",
    "\n",
    "Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
